{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Natural Language Processing\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Entity finder in news articles\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Author: Sandra Vald√©s Salas\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "March 2020\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Table of Contents_\n",
    "\n",
    "* [Step 1) Import necessary libraries](#1)\n",
    "* [Step 2) Fucntion to exrtact entity, document id and relevant sentence text](#2)\n",
    "* [Step 3) Call the entity extraction function and add the entity records to a combined dictionary](#3)\n",
    "* [Step 4) Function to determine the most popular entity based on number of mentions](#4)\n",
    "* [Step 5) Invoke the top entity mention finder](#1)\n",
    "* [Step 6) Analyze the most popular entities to determine what words they most frequently occur with](#6)\n",
    "* [Step 7) Results](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Introduction_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise extracts the entities (Person, Organizations, Locations, Geopolitical Entities) from Reuters news articles. The top entities from each category are found. NLTK and SpaCy are used for this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Import necessary libraries <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk and spacy\n",
    "import nltk.data\n",
    "from nltk.corpus import reuters\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Function to extract the entity, document id, and relevant sentence text <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extract_entities` function returns a dictionary with all the entities extracted from the input text. The values of each key-entity corresponds to a tuple, in which the first element is the document id and the second element is a list of all sentences that mention the entity in the same document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to process all labels and dictionaries\n",
    "\n",
    "def process_dict(entity, relevant_sentence, label, dictionary):\n",
    "    '''In case an entity occurs multiple times in the same document, \n",
    "    this function creates a key,value pair where the key = entity and\n",
    "    value = tuple with document id and a list of all sentences where the entity is mentioned'''\n",
    "    \n",
    "    if entity.label_==label and entity.text.strip() not in dictionary:\n",
    "        dictionary[entity.text.strip()]=relevant_sentence \n",
    "    elif entity.label_==label and entity.text.strip() in dictionary: \n",
    "        dictionary[entity.text.strip()][1].append(entity.sent.text.strip())\n",
    "        \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc_id, doc_text):\n",
    "    analyzed_doc = nlp(doc_text)\n",
    "    doc_persons = {}\n",
    "    doc_organizations = {}\n",
    "    doc_locations = {}\n",
    "    doc_gpe = {}\n",
    "    \n",
    "    for entity in analyzed_doc.ents:\n",
    "        if entity.text.strip() != \"\":\n",
    "            #print(\" -> \", entity.label_)\n",
    "            #print(\"->\", entity.text.strip(), \"<-\")\n",
    "            #print(\"->\", entity.sent.text.strip(), \"<-\")\n",
    "            #Use lower cases to avoid repited entities, like \"FED\" and \"Fed\"\n",
    "            relevant_sentence = (doc_id, [entity.sent.text.strip()])\n",
    "            #Use process_dict function to create the dictionaries for each entity\n",
    "            #The value corresponds to a tuple = (document_id, list of all entity mentions in the document)\n",
    "            process_dict(entity,relevant_sentence,\"PERSON\",doc_persons)\n",
    "            process_dict(entity,relevant_sentence,\"ORG\",doc_organizations)\n",
    "            process_dict(entity,relevant_sentence,\"LOC\",doc_locations)\n",
    "            process_dict(entity,relevant_sentence,\"GPE\",doc_gpe)\n",
    "            \n",
    "    return doc_persons, doc_organizations, doc_locations, doc_gpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a small sample, we can see that the function is working. In the case of GPE entities, the U.S. is mentioned in different sentences of the same document. So the function works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test/14826',\n",
       " ['Mounting trade friction between the\\n  U.S.',\n",
       "  'They told Reuter correspondents in Asian capitals a U.S.',\n",
       "  'Move against Japan might boost protectionist sentiment in the\\n  U.S. And lead to curbs on American imports of their products.',\n",
       "  \"The U.S. Has said it will impose 300 mln dlrs of tariffs on\\n  imports of Japanese electronics goods on April 17, in\\n  retaliation for Japan's alleged failure to stick to a pact not\\n  to sell semiconductors on world markets at below cost.\",\n",
       "  '\"If the tariffs remain in place for any length of time\\n  beyond a few months it will mean the complete erosion of\\n  exports (of goods subject to tariffs) to the U.S.,\" said Tom\\n  Murtha, a stock analyst at the Tokyo office of broker &lt;James\\n  Capel and Co>.',\n",
       "  '\"We are aware of the seriousness of the U.S. Threat against\\n  Japan because it serves as a warning to us,\" said a senior\\n  Taiwanese trade official who asked not to be named.',\n",
       "  'Taiwan had a trade trade surplus of 15.6 billion dlrs last\\n  year, 95 pct of it with the U.S.',\n",
       "  '\"We must quickly open our markets, remove trade barriers and\\n  cut import tariffs to allow imports of U.S. Products, if we\\n  want to defuse problems from possible U.S. Retaliation,\" said\\n  Paul Sheen, chairman of textile exporters &lt;Taiwan Safe Group>.',\n",
       "  \"A senior official of South Korea's trade promotion\\n  association said the trade dispute between the U.S.\",\n",
       "  'Last year South Korea had a trade surplus of 7.1 billion\\n  dlrs with the U.S., Up from 4.9 billion dlrs in 1985.',\n",
       "  'In Malaysia, trade officers and businessmen said tough\\n  curbs against Japan might allow hard-hit producers of\\n  semiconductors in third countries to expand their sales to the\\n  U.S.',\n",
       "  'But other businessmen said such\\n  a short-term commercial advantage would be outweighed by\\n  further U.S. Pressure to block imports.',\n",
       "  'The U.S.',\n",
       "  'The Australian government is awaiting the outcome of trade\\n  talks between the U.S. And Japan with interest and concern,\\n  Industry Minister John Button said in Canberra last Friday.',\n",
       "  \"Australia's two largest exports to Japan and also significant\\n  U.S. Exports to that country.\",\n",
       "  \"Deputy U.S. Trade Representative Michael Smith and Makoto\\n  Kuroda, Japan's deputy minister of International Trade and\\n  Industry (MITI), are due to meet in Washington this week in an\\n  effort to end the dispute.\"])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_doc = reuters.open('test/14826').read()\n",
    "sample_per, sample_org, sample_loc, sample_gpe = extract_entities('test/14826',sample_doc)\n",
    "#sample_per  #sample to see check that the previous functions work\n",
    "sample_gpe[\"U.S.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Call the entity extraction function and add the entity records to a combined dictionary <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs = len(reuters.fileids())\n",
    "num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get files from reuters\n",
    "reuters_files = reuters.fileids()\n",
    "#reuters_files = reuters.fileids()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combined_dict(key, dictionary):\n",
    "    '''This function adds a list of values to the keys of the combined dictionary. In case the \n",
    "    key already exists, the new elements will be the list of values'''\n",
    "    for key,value in key.items():\n",
    "        if key not in dictionary:\n",
    "            dictionary[key]=[value]\n",
    "        else:\n",
    "            dictionary[key].append(value)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_persons = {}\n",
    "combined_organizations = {}\n",
    "combined_locations = {}\n",
    "#combined_gpe = {}\n",
    "\n",
    "for doc_id in reuters_files:\n",
    "    #extract the input and use lower case to avoid repetition in entities like \"FED\" and \"Fed\"\n",
    "    persons, organizations, locations, gpe=extract_entities(doc_id, reuters.open(doc_id).read().replace('\\n', '').lower())\n",
    "    #Use process_combined_dict function to add new items to combined dictionaries\n",
    "    process_combined_dict(persons,combined_persons)\n",
    "    process_combined_dict(locations,combined_locations)\n",
    "    process_combined_dict(organizations,combined_organizations)\n",
    "    #process_combined_dict(gpe,combined_gpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4070"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_persons.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_locations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15697"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_organizations.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep in mind the structure of the combined dictionary\n",
    "\n",
    "#entity : [list of tuples]\n",
    "#each tuple = (document_id, [sentence 1, sentence 2, etc...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Function to determine the most popular entity based on number of mentions <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Function to get a list of tuples with the most mentioned entities and their total mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_mentions_tuples(entity_dictionary):\n",
    "    most_common=[]\n",
    "    \n",
    "    #iterate dictionary key\n",
    "    for key in entity_dictionary.keys():\n",
    "        total_entity_mentions=0\n",
    "        #iterate each tuple with info about (doc_id, sentences)\n",
    "        for t in range(len(entity_dictionary[key])):\n",
    "            #total_entity_mentions += len(entity_dictionary[key][t]) -> get documents\n",
    "            total_entity_mentions += len(entity_dictionary[key][t][1]) #get all mentions per document\n",
    "        #get tuples -> (entity, total mentions in all documents)\n",
    "        most_common.append((key, total_entity_mentions))\n",
    "    \n",
    "    #sort entities based on total number of mentions\n",
    "    most_common= sorted(most_common, key = lambda x:x[1], reverse=True)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reagan', 409), ('baker', 278), ('lawson', 123)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 3 entities - persons mentioned in the corpus, with the frequencies of their mentions\n",
    "tuples_persons = entity_mentions_tuples(combined_persons)\n",
    "tuples_persons[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Function to get a list of top 500 entities with the most mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_popular_entities(entity_dictionary):\n",
    "    #Get list of tuples (entity, total mentions)\n",
    "    tuples_with_most_mentions= entity_mentions_tuples(entity_dictionary)\n",
    "    #get top 50\n",
    "    return [t[0] for t in tuples_with_most_mentions[:500]]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Invoke the top entity mention finder <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = find_most_popular_entities(combined_persons)\n",
    "top_locations = find_most_popular_entities(combined_locations)\n",
    "top_organizations = find_most_popular_entities(combined_organizations)\n",
    "#top_gpe = find_most_popular_entities(combined_gpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6) Analyze the most popular entities to determine what words they most frequently occur with <a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dictionary has a key, value pair where the key is the entity and the value is a tuple. Each tuple has: 1) the document id; 2) a list of all sentences related to the entity. It may be the case that an entity appears many times in the same document, therefore, the this method considers that followins steps:\n",
    "\n",
    "1. Extracts all sentences from the tuple.\n",
    "2. Creates a single string with all the sentences and store it in an object called \"text\".\n",
    "3. Tokenizes the object text.\n",
    "4. Returns tokens that are not numeric, not stopwords.\n",
    "5. Tokens such as \"say\" or \"said\" are not considered (which I assume will have a higher frequency and are irrelevant for the analysis).\n",
    "6. Finally, only tokens that are different from the entity text are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get all tokens of an entity\n",
    "def get_tokens(dictionary):\n",
    "    '''Join all sentences (mentions of the entity) and extract the most relevant tokens:\n",
    "    not stop word, not numeric, not equal to the entity, not repetitive words like \"say\" or \"said\"'''\n",
    "    text=\"\"\n",
    "    for item in range(len(dictionary[ent])):\n",
    "        mentions = dictionary[ent][item][1]\n",
    "        all_mentions=\"\".join(mentions)\n",
    "        text += all_mentions\n",
    "    doc = nlp(text)\n",
    "    #return [t.text for t in doc if not t.is_stop if t.is_alpha if t.pos_!=\"VERB\" if t.text!=ent]\n",
    "    return [t.text for t in doc if not t.is_stop if t.is_alpha if t.text!=ent if t.text!=\"says\" if t.text!=\"said\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main dictionary -> keys are entities and values are dictionaries of top 50 words with their frequencies \n",
    "person_most_popular_terms = {}\n",
    "\n",
    "for ent in top_persons:\n",
    "    person_token_dictionary = {}\n",
    "    #get all tokens \n",
    "    person_words = get_tokens(combined_persons)\n",
    "    #get frequencies\n",
    "    for w in person_words:\n",
    "        if w not in person_token_dictionary.keys():\n",
    "            person_token_dictionary[w]=1\n",
    "        else:\n",
    "            person_token_dictionary[w]+=1\n",
    "    \n",
    "    #sort tokens_freq dicionary based on value\n",
    "    person_sorted_words= sorted(person_token_dictionary.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    #person_top_words = dict([t for t in person_sorted_words[:25]]) #store words,freq as dictionary\n",
    "    person_top_words = [t[0] for t in person_sorted_words[:50]] #store words as list\n",
    "    \n",
    "    #store results as values in main dictionary\n",
    "    person_most_popular_terms[ent] = person_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_most_popular_terms = {}\n",
    "\n",
    "for ent in top_organizations:\n",
    "    \n",
    "    organization_token_dictionary = {}\n",
    "    \n",
    "    org_words = get_tokens(combined_organizations)\n",
    "    for w in org_words:\n",
    "        if w not in organization_token_dictionary.keys():\n",
    "            organization_token_dictionary[w]=1\n",
    "        else:\n",
    "            organization_token_dictionary[w]+=1\n",
    "            \n",
    "    org_sorted_words= sorted(organization_token_dictionary.items(), key=lambda x:x[1], reverse=True)\n",
    "    org_top_words = [t[0] for t in org_sorted_words[:50]] \n",
    "    organization_most_popular_terms[ent] = org_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_most_popular_terms = {}\n",
    "\n",
    "for ent in top_locations:\n",
    "    \n",
    "    location_token_dictionary = {}\n",
    "    \n",
    "    loc_words = get_tokens(combined_locations)\n",
    "    for w in loc_words:\n",
    "        if w not in location_token_dictionary.keys():\n",
    "            location_token_dictionary[w]=1\n",
    "        else:\n",
    "            location_token_dictionary[w]+=1\n",
    "            \n",
    "    loc_sorted_words= sorted(location_token_dictionary.items(), key=lambda x:x[1], reverse=True)\n",
    "    loc_top_words = [t[0] for t in loc_sorted_words[:50]] \n",
    "    location_most_popular_terms[ent] = loc_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpe_most_popular_terms = {}\n",
    "#for ent in top_gpe:    \n",
    "    #gpe_token_dictionary = {}\n",
    "    #gpe_words = get_tokens(combined_gpe)\n",
    "    #for w in gpe_words:\n",
    "       # if w not in gpe_token_dictionary.keys():\n",
    "          #  gpe_token_dictionary[w]=1\n",
    "        #else:\n",
    "          #  gpe_token_dictionary[w]+=1       \n",
    "    #gpe_sorted_words= sorted(gpe_token_dictionary.items(), key=lambda x:x[1], reverse=True)\n",
    "    #gpe_top_words = [t[0] for t in gpe_sorted_words[:50]] \n",
    "    #gpe_most_popular_terms[ent] = gpe_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7) Results  <a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1) Persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common entities with their frequencies (total of mentions in all documents of Reuters corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reagan', 409),\n",
       " ('baker', 278),\n",
       " ('lawson', 123),\n",
       " ('yeutter', 121),\n",
       " ('james baker', 94),\n",
       " ('baldrige', 66),\n",
       " ('johnson', 64),\n",
       " ('mln', 63),\n",
       " ('poehl', 54),\n",
       " ('stoltenberg', 53)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples_persons = entity_mentions_tuples(combined_persons)\n",
    "tuples_persons[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documents with the entity mentioned\n",
    "len(combined_persons['reagan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documents with the entity mentioned\n",
    "len(combined_persons['baker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0485724879495737"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total mentions as a percentage of the corpus length \n",
    "len(combined_persons['reagan'])/len(reuters_files)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **'reagan'** is the most popular entity in persons dictionary. It is mentioned 409 times in the Reuters corpus (409 sentences have \"reagan\"). Also, it appears in 221 documents, which accounts for 2% of the entire Reuters corpus.\n",
    "- **'baker'** is the second most mentioned entity. It is mentioned 278 times in the Reuters corpus, and it appears in 93 documents of the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **top 500 entities** with the largest number of mentions in the **persons dictionary**, let's take a look at the first 100 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reagan', 'baker', 'lawson', 'yeutter', 'james baker', 'baldrige', 'johnson', 'mln', 'poehl', 'stoltenberg', '1986/87', 'inra', 'clayton yeutter', 'kiichi miyazawa', 'herrington', 'williams', 'heller', 'yasuhiro nakasone', 'richard lyng', 'satoshi sumita', 'nigel lawson', 'chirac', 'clark', 'edouard balladur', 'clayton', 'jardine matheson', 'paul volcker', 'malcolm baldrige', 'caspar weinberger', 'twa', 'nazer', 'donald trump', '1987/88', 'bass', 'gerhard stoltenberg', 'howard', 'microchips', 'simon', 'amc', 'mulroney', 'greenspan', 'george shultz', 'de clercq', 'ali', 'jorio dauster', 'richard  lyng', 'rotterdam', 'kim', 'karl otto poehl', 'heyman', 'marlin fitzwater', 'margaret thatcher', 'darman', 'leigh-pemberton', 'subroto', 'fitzwater', 'bangemann', 'paul  volcker', 'silas', 'karl otto', 'harris', 'jordan', 'nspa', 'gao', 'john herrington', 'kato', 'corazon aquino', 'gerhard  stoltenberg', 'morgan', 'gelco', 'barber', 'sama', 'marshall', 'brown', 'romero', 'lee', 'jim wright', 'durham', 'dan glickman', 'jacques chirac', 'martin sosnoff', 'roderick', 'yulo', 'sarney', 'gaviria', 'wendy', 'rexham', 'mccarthy', 'conger', 'hisham nazer', 'lukman', 'indonesia', 'carl icahn', 'gencorp', 'hutchison', 'van driel', 'li', 'xuto', 'atlas', 'stevens']\n"
     ]
    }
   ],
   "source": [
    "print(top_persons[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overall, it seems that the method worked. However, some of these entities are repeated. For example, we have 2 different entities related to James Baker: **\"baker\"** and **\"james baker\"**. It is important to figure out a method that will treat both entities as the same person. This is a key point because it will change the total count of sentences associated to these entities and, thus, our rankings will be adjusted. \n",
    "- Also, we should avoid the entity '1987/88', which clearly is not a person. \n",
    "\n",
    "We can take a look at the most frequent tokens associated with each entity in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['administration', 'trade', 'president', 'japan', 'oil', 'economic', 'bill', 'japanese', 'house', 'foreign', 'congress', 'gulf', 'tariffs', 'united', 'states', 'policy', 'officials', 'agreement', 'tax', 'secretary', 'year', 'official', 'dlrs', 'legislation', 'wheat', 'decision', 'markets', 'unfair', 'action', 'soviet', 'new', 'today', 'retaliate', 'senate', 'union', 'sanctions', 'offer', 'semiconductor', 'retaliation', 'exports', 'mln', 'open', 'week', 'industry', 'impose', 'help', 'opposition', 'countries', 'american', 'washington']\n"
     ]
    }
   ],
   "source": [
    "#The top 50 tokens associated with the entity \"reagan\"\n",
    "print(person_most_popular_terms['reagan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **administration**, **trade** and **president** are the most frequent words associated with **'reagan'** (in that order). \n",
    "- Overall, the rest of the words are related to political economy (tariffs, sanctions, exports), legislation (tax, policy), domestic politics (congress, opposition, union) and international politics (gulf, soviet, foreign). \n",
    "- Some words have negative connotation: retaliation, unfair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['west', 'trade', 'hughes', 'treasury', 'agreement', 'dollar', 'louvre', 'meeting', 'rate', 'exchange', 'german', 'merger', 'paris', 'policy', 'interest', 'currency', 'accord', 'economic', 'agreed', 'germany', 'department', 'sees', 'rates', 'international', 'comment', 'deficit', 'secretary', 'told', 'monetary', 'weekend', 'stoltenberg', 'billion', 'consent', 'reduction', 'currencies', 'dlr', 'justice', 'decree', 'japanese', 'james', 'remarks', 'today', 'interview', 'declined', 'market', 'markets', 'official', 'proposed', 'imbalances', 'finance']\n"
     ]
    }
   ],
   "source": [
    "#The top 50 tokens associated with the entity \"reagan\"\n",
    "print(person_most_popular_terms['baker'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **imbalances**, **rate** and **hughes** are the most frequent words associated with **'baker'** (in that order). \n",
    "- Overall, the rest of the words are related to economic jargon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trump', 'donald', 'estate', 'resorts', 'dlrs', 'real', 'developer', 'inc', 'stock', 'ual', 'crosby', 'shares', 'mln', 'new', 'york', 'international', 'casino', 'class', 'b', 'common', 'agreed', 'purchase', 'hotel', 'month', 'held', 'bid', 'acquire', 'control', 'buy', 'interested', 'interstate', 'alexanders', 'discussions', 'takeover', 'recently', 'apparently', 'unsuccessful', 'spokesman', 'sharesdonald', 'family', 'chairman', 'james', 'manufacturing', 'charge', 'earnings', 'quarter', 'result', 'deal', 'bally', 'according']\n"
     ]
    }
   ],
   "source": [
    "print(person_most_popular_terms['donald trump'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some entities are written differently but refer to the same person, as is the case with **\"Trump\"** and **\"Donald Trump\"**. \n",
    "- We can find the union of the 2 lists (although the result will not be a list of words, orderd by frequency). \n",
    "\n",
    "As a result, these are the most frequent words (unorodered) associated with \"Trump\" and \"Donald Trump\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['takeover', 'class', 'alexanders', 'new', 'buy', 'resorts', 'quarter', 'international', 'offered', 'stock', 'sharesdonald', 'held', 'real', 'makes', 'agreed', 'unsuccessful', 'common', 'bid', 'discussions', 'james', 'month', 'family', 'inc', 'deal', 'developer', 'shares', 'spokesman', 'charge', 'bally', 'seek', 'price', 'crosby', 'york', 'donald', 'estate', 'hotel', 'b', 'owner', 'acquire', 'remaining', 'level', 'dlrs', 'casino', 'mln', 'rival', 'recently', 'earnings', 'result', 'purchase', 'pratt', 'reach', 'manufacturing', 'requires', 'interested', 'according', 'trump', 'control', 'apparently', 'ual', 'february', 'chairman', 'investment', 'offer', 'try', 'corp', 'share', 'beat', 'interstate']\n"
     ]
    }
   ],
   "source": [
    "a = person_most_popular_terms['donald trump']\n",
    "b = person_most_popular_terms['trump']\n",
    "print(list(set().union(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2) Organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common entities with their frequencies (total of mentions in all documents of Reuters corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mln', 5984),\n",
       " ('cts', 4936),\n",
       " ('pct', 2477),\n",
       " ('ec', 871),\n",
       " ('fed', 663),\n",
       " ('opec', 524),\n",
       " ('reuters', 442),\n",
       " ('treasury', 442),\n",
       " ('usda', 367),\n",
       " ('bundesbank', 304)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most common entities with their frequencies (total of mentions in all documents from entire corpus)\n",
    "tuples_organizations = entity_mentions_tuples(combined_organizations)\n",
    "tuples_organizations[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **'mln'** is a popular entity in organizations dictionary, followed by 'cts' and 'pct'. These entities might not necessary suggest a valid organization (for example, is probable that 'mln' is million). Also, these entities have a very large frequency, so it is probable that they might refer to other concepts different from *organizations*. \n",
    "- By contrast, 'fed', 'opec', 'reuters', and the rest of the entities, are known organizations.  note that **'fed'** is mentioned 663 times in the entire corpus, followed by **'opec'** which is mentioned 524 times in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oil', 'prices', 'mln', 'bpd', 'production', 'output', 'price', 'dlrs', 'official', 'december', 'market', 'barrel', 'crude', 'quota', 'members', 'barrels', 'meeting', 'ceiling', 'february', 'day', 'arabia', 'minister', 'saudi', 'countries', 'agreed', 'year', 'agreement', 'world', 'demand', 'al', 'member', 'fixed', 'president', 'ecuador', 'conference', 'producing', 'dlr', 'sources', 'spot', 'levels', 'traders', 'new', 'march', 'energy', 'quotas', 'group', 'quarter', 'lukman', 'current', 'level']\n"
     ]
    }
   ],
   "source": [
    "print(organization_most_popular_terms['opec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected, the most frequent words related to the **OPEC** organization are words related to oil production and oil market: oil, prices, production, barrel, crude, arabia, energy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['billion', 'rates', 'marks', 'money', 'market', 'central', 'bank', 'rate', 'interest', 'president', 'pct', 'liquidity', 'poehl', 'german', 'dealers', 'west', 'cut', 'banks', 'repurchase', 'credit', 'monetary', 'week', 'karl', 'otto', 'term', 'february', 'schlesinger', 'policy', 'dollar', 'council', 'meeting', 'securities', 'policies', 'tender', 'short', 'mark', 'currency', 'today', 'fixed', 'net', 'growth', 'lower', 'germany', 'yen', 'january', 'pact', 'spokesman', 'funds', 'month', 'supply']\n"
     ]
    }
   ],
   "source": [
    "print(organization_most_popular_terms['bundesbank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected, the most frequent words associated to **Bundesbank** are words related to finance and economics, such as: rate, money, market, credit, germany."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3) Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common entities with their frequencies (total of mentions in all documents of Reuters corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gulf', 248),\n",
       " ('europe', 169),\n",
       " ('west texas', 49),\n",
       " ('north sea', 41),\n",
       " ('asia', 40),\n",
       " ('africa', 40),\n",
       " ('the middle east', 32),\n",
       " ('north america', 30),\n",
       " ('west', 30),\n",
       " ('atlantic', 23)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most common entities with their frequencies (total of mentions in all documents from entire corpus)\n",
    "tuples_locations = entity_mentions_tuples(combined_locations)\n",
    "tuples_locations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documents with the entity mentioned\n",
    "len(combined_locations['gulf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documents with the entity mentioned\n",
    "len(combined_locations['europe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **'gulf'** is the most popular entity: there are 248 sentences that mention 'gulf' in the corpus. Also, it is mentioned in 109 documents. \n",
    "- **'europe'** has also a high frequency: there are 169 sentences that mention 'europe' in the corpus. However, it appears in more documents (124 documents) than 'gulf'.\n",
    "- The rest of the locations have a frequency lower than 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **top 500 entities** with the largest number of mentions in the **locations dictionary**, let's take a look at the first 100 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gulf', 'europe', 'west texas', 'north sea', 'asia', 'africa', 'the middle east', 'north america', 'west', 'atlantic', 'midwest', 'the  gulf', 'pacific', 'mideast', '1986/87', 'middle east', 'western europe', 'mediterranean', 'the far east', 'the north sea', 'latin america', 'south america', 'west coast', 'northeast', 'the gulf of mexico', 'nova', '4th qtr', 'mississippi river', 'new england', 'southern gulf', 'the aegean sea', 'eastern europe', 'the mideast gulf', 'east', 'the pacific coast', 'east coast', 'the red river', 'the east coast', 'south', 'southeast asia', 'valley', 'southern california', 'the west coast', 'kharg island', 'continental europe', 'la pampa', 'highland valley', 'east bloc', 'east europe', 'mideast gulf', 'caribbean', 'scandinavia', 'the persian gulf', 'western', 'the southern gulf', 'south louisiana', 'the black sea', 'eastern bloc', 'lake ontario', 'the north china', 'delta', 'midmississippi river', 'illinois river', 'the mississippi river', 'persian gulf', 'north yemen', 'the northern arabian sea', 'west   europe', '3rd qtr', 'south central', 'jupiter', 'south china', 'central kansas', 'south-east asia', 'the gulf area', 'southern pacific', 'ketza river', 'south wales', 'lake erie', 'echo bay', 'earth', 'east china', 'central america', 'south china sea', 'hudson bay', 'south asia', 'greens creek', 'eastern thailand', 'san joaquin valley', 'trans-atlantic', 'southern italy', 'east central', 'south east asia', 'south atlantic', 'gulf coast', 'mount', 'prudhoe bay', 'the gulf arab states', 'the south pacific', 'southern europe']\n"
     ]
    }
   ],
   "source": [
    "print(top_locations[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overall, many locations are extracted in 2 ways: \"the east cost\" and \"east cost\". Therefore, many of these words are counted separately, although they refer to the same location. As a possible solution, we could delete \"the\" from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oil', 'iranian', 'states', 'iran', 'united', 'attack', 'shipping', 'reagan', 'military', 'mln', 'dlrs', 'missiles', 'prices', 'kuwaiti', 'american', 'price', 'warned', 'new', 'told', 'tehran', 'minister', 'forces', 'ships', 'tankers', 'ship', 'protect', 'use', 'monday', 'platform', 'attacks', 'tension', 'president', 'kuwait', 'arab', 'action', 'officials', 'fob', 'foreign', 'soviet', 'rates', 'week', 'hormuz', 'corn', 'usda', 'barge', 'freight', 'near', 'anti', 'region', 'iraq']\n"
     ]
    }
   ],
   "source": [
    "print(location_most_popular_terms['gulf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **'gulf'** location has words related to oil production, international politics (in specific, U.S.-Middle East politics), an military jargon (attacks, tension, tankers, forces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mln', 'oil', 'japan', 'market', 'year', 'japanese', 'exports', 'currency', 'pct', 'european', 'growth', 'trade', 'export', 'united', 'states', 'company', 'imports', 'prices', 'west', 'america', 'south', 'east', 'sales', 'demand', 'dlrs', 'largest', 'world', 'rose', 'sold', 'rate', 'billion', 'far', 'chairman', 'interest', 'report', 'sharply', 'added', 'firms', 'domestic', 'officials', 'ec', 'yen', 'rise', 'high', 'dollar', 'crude', 'economic', 'terms', 'cost', 'help']\n"
     ]
    }
   ],
   "source": [
    "print(location_most_popular_terms['europe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **'europe'** has words related to macro economics (exports, growth, trade, imports); others refer to european commerce relations with other states (specifically Japan and the U.S.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "Some additional coding to improve the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Determine which persons, organizations and locations most frequently occur in the same sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to approach this problem is by calculating the frequency of each entity in a sentence, store the entity, frequency in a dictionary (as key, value pairs). We could then sort the dictionary items based on the values. We can apply this method to the combined dictionaries of persons, organizations, locations.\n",
    "\n",
    "The following is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A U.S. move against Japan might boost protectionist sentiment in the U.S. against Japan and lead to curbs on american imports of their products, said the chairman of the Committee of Commerce in the U.S., Roger F. Wicker."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count entities with the largest number of mentions in the same sentence, for example:\n",
    "sample=\"A U.S. move against Japan might boost protectionist sentiment in the U.S. against Japan and lead to curbs on american imports of their products, said the chairman of the Committee of Commerce in the U.S., Roger F. Wicker.\"\n",
    "doc = nlp(sample)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. GPE\n",
      "Japan GPE\n",
      "U.S. GPE\n",
      "Japan GPE\n",
      "american NORP\n",
      "the Committee of Commerce ORG\n",
      "U.S. GPE\n",
      "Roger F. Wicker PERSON\n"
     ]
    }
   ],
   "source": [
    "#Print all entities of the sentence with its label\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'U.S.': 3,\n",
       " 'Japan': 2,\n",
       " 'american': 1,\n",
       " 'the Committee of Commerce': 1,\n",
       " 'Roger F. Wicker': 1}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of times the entity appears in the sentence and store it in a dictionary\n",
    "ent_counter={}\n",
    "for ent in doc.ents:\n",
    "    if ent.text not in ent_counter:\n",
    "        ent_counter[ent.text]=1\n",
    "    else: \n",
    "        ent_counter[ent.text]+=1\n",
    "ent_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.', 3),\n",
       " ('Japan', 2),\n",
       " ('american', 1),\n",
       " ('the Committee of Commerce', 1),\n",
       " ('Roger F. Wicker', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort the dictionary based on the values of the entities \n",
    "#and return a list of tuples with the (entity, frequency)\n",
    "sorted_entities = sorted(ent_counter.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U.S.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Or we can simply get the most repeated element in the sentence\n",
    "sorted_entities[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the U.S. GPE entity has the largest number of mentions in the sentence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
