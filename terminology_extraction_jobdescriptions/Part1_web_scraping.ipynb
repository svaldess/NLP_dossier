{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"NLP_project_part1_final.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dYF1QMJyCgni","colab_type":"text"},"source":["<b>\n","\n","<p>\n","<center>\n","<font size=\"5\">\n","Natural Language Processing (Spring 2020)\n","</font>\n","</center>\n","</p>\n","\n","<p>\n","<center>\n","<font size=\"4\">\n","PROJECT: AUTOMATIC TERMINOLOGY ANALYSIS FOR DATA SCIENCE JOB DESCRIPTIONS \n","</font>\n","</center>\n","</p>\n","\n","<p>\n","<center>\n","<font size=\"3\">\n","Authors: Anwesha Tomar, Marta Matosas Fonolleda, Sandra Valdes Salas\n","</font> \n","</center>\n","</p>\n","\n","<p>\n","<center>\n","<font size=\"3\">\n","Part 1: Web scraping code\n","</font> \n","</center>\n","</p>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kdHuVHTluw8D","colab_type":"text"},"source":["### Setup Google Colab"]},{"cell_type":"code","metadata":{"id":"-DRZukj0u0GW","colab_type":"code","colab":{}},"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJD9xZ6ou1pU","colab_type":"code","colab":{}},"source":["#%cd /content/drive/My\\ Drive/Colab\\ Notebooks/NLP"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRxuH1NlCgnj","colab_type":"text"},"source":["# 1) Import libaries"]},{"cell_type":"code","metadata":{"id":"zNO7g1hECgnl","colab_type":"code","colab":{}},"source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.request import urlopen \n","import re\n","from IPython.core.display import clear_output\n","import pandas as pd\n","from time import time\n","from time import sleep\n","from random import randint"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_5-_bcvCgnp","colab_type":"text"},"source":["# 2) Build functions for web scraping "]},{"cell_type":"markdown","metadata":{"id":"TTkmflliCgnp","colab_type":"text"},"source":["## 2.1 Convert URL based on query (position and city)"]},{"cell_type":"code","metadata":{"id":"UeuxreWUCgnq","colab_type":"code","colab":{}},"source":["def convert_url(position, city):\n","    '''\n","    Function converts base url according to query\n","    Returns: string\n","    '''\n","    position = position.replace(' ','+')\n","    city = city.replace(\" \",\"+\")\n","    search_url = \"https://www.indeed.com/jobs?q={}&l={}&sort=date\"\n","    return search_url.format(position, city)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4RYyroOCgnt","colab_type":"code","outputId":"e1fca3b6-d2c5-4327-858b-bd0b5a962e04","colab":{}},"source":["#Sample\n","convert_url('data science', 'washington dc')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'https://www.indeed.com/jobs?q=data+science&l=washington+dc&sort=date'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"e3Hky_soCgnw","colab_type":"text"},"source":["## 2.2 Get total pages of query"]},{"cell_type":"code","metadata":{"id":"R9xZgaweCgnx","colab_type":"code","colab":{}},"source":["def get_max_pages(search_url):\n","    '''\n","    Function calculates maximum number of pages of search url\n","    Returns: integer\n","    '''\n","    url = search_url+\"&start=00\"\n","    r = requests.get(url)\n","    soup = BeautifulSoup(r.content, 'lxml')\n","    job_link_area = soup.find(id = 'resultsCol')\n","    pages = job_link_area.find(\"div\", { \"id\" : \"searchCountPages\" }).text.strip().split()[3].replace(',','')\n","    max_pages = int(int(pages)/10)*10\n","    if max_pages > 1000:\n","        return 990\n","    else:\n","        return max_pages"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GX0sJlihCgn2","colab_type":"code","outputId":"70c9fc83-ff12-4eba-f854-9c1f1f5bd1eb","colab":{}},"source":["#Sample\n","get_max_pages('https://www.indeed.com/jobs?q=data+science&l=washington+dc&sort=date')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["990"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"MPKD6ESpCgn5","colab_type":"code","outputId":"49f7d380-4c94-4c60-f2ef-8adc40393637","colab":{}},"source":["#Sample\n","search_url = 'https://www.indeed.com/jobs?q=data+science&l=washington+dc&sort=date'\n","page_url = search_url +\"&start=\" +str(10)\n","page_url"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'https://www.indeed.com/jobs?q=data+science&l=washington+dc&sort=date&start=10'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"QqlTH3OyCgn8","colab_type":"text"},"source":["## 2.3 Set crawler: get all job url's displayed in a page "]},{"cell_type":"code","metadata":{"id":"_UTJoGS1Cgn9","colab_type":"code","colab":{}},"source":["def get_job_urls(soup_obj, base_url='https://www.indeed.com'):\n","    '''\n","    Function extracts job url link for each job posted in the web page\n","    Returns: list of url (string)\n","    '''\n","    #base_url = base_url\n","    job_link_area=soup_obj.find(id = 'resultsCol')\n","    job_urls = []\n","    for a in [link.find_all('a') for link in job_link_area.find_all('h2') if link.get('class') == ['title']]:\n","        job_urls.append(base_url+a[0].get('href'))\n","    return job_urls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x91mKbRYCgoB","colab_type":"code","outputId":"facbc226-89ff-4d03-f26c-e42d5ebebe8c","colab":{}},"source":["# Sample\n","base_url_page = 'https://www.indeed.com'\n","url_page = 'https://www.indeed.com/jobs?q=data+science&l=washington+dc&sort=date&start=20'\n","r = requests.get(url_page)\n","soup = BeautifulSoup(r.content, 'lxml')\n","jobs_urls = get_job_urls(soup)\n","jobs_urls[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'https://www.indeed.com/rc/clk?jk=bb6228270244aeae&fccid=05d6cb8b919478a9&vjs=3'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ZTUtW7E0CgoE","colab_type":"text"},"source":["## 2.4 Extract job information"]},{"cell_type":"markdown","metadata":{"id":"xFwuAL8sCgoE","colab_type":"text"},"source":["### 2.4.1 Extract business sector for each job posting"]},{"cell_type":"code","metadata":{"id":"VeImQAl4CgoF","colab_type":"code","colab":{}},"source":["#extract business sector \n","def fetch_sector(soup_obj):\n","    '''\n","    Function gets link associated to company information and get sector of the company.\n","    Returns: string\n","    '''\n","    try:\n","        company_link=soup_obj.find('div', class_=\"icl-u-lg-mr--sm icl-u-xs-mr--xs\").find('a').get('href')\n","        page = urlopen(company_link) \n","        soup = BeautifulSoup(page)\n","        return soup.find('a', class_=\"cmp-AboutMetadata-plainLink\").text\n","    except:\n","        return None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O83BqrA9CgoJ","colab_type":"text"},"source":["### 2.4.2 Create dictionary with url, position, company, description, sector for each job posting"]},{"cell_type":"code","metadata":{"id":"7GQ4u-JcCgoK","colab_type":"code","colab":{}},"source":["def extract_job_information(job_url):\n","    '''\n","    Function extracts all information from a given job url.\n","    Returns: dictionary\n","    '''\n","    job={}\n","    page =  urlopen(job_url)\n","    soup = BeautifulSoup(page)\n","    \n","    job['url']=job_url \n","    job['position'] = soup.find('div', class_='jobsearch-JobInfoHeader-title-container').text\n","    try:\n","        job['company'] = soup.find('div', class_ = 'icl-u-lg-mr--sm icl-u-xs-mr--xs').text\n","    except: \n","        job['company'] = None\n","    job['description'] = soup.find('div', class_='jobsearch-jobDescriptionText').text\n","    job['sector'] = fetch_sector(soup)\n","    return job\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdAGOrwZCgoS","colab_type":"text"},"source":["## 2.5 Function for web scraping"]},{"cell_type":"code","metadata":{"id":"rTHIZsbaCgoS","colab_type":"code","colab":{}},"source":["def job_scraper(position, city):\n","    '''\n","    Function scrapes Indeed web page based on query position and city and extracts \n","    raw html for every job position found in the page.\n","    Returns: dictionary\n","    '''\n","    # Prepare url\n","    url_search = convert_url(position, city)\n","    \n","    # Get max pages\n","    #max_pages = get_max_pages(url_search)\n","    max_pages = 10 #sample\n","    \n","    # set counters\n","    #start_time = time()\n","    req = 0 #requests counter\n","    jobs_extracted = 0 # jobs counter\n","\n","    data=[]\n","    \n","    # iterate through each page\n","    for num in range(0, max_pages, 10):\n","        url_current_page = url_search +\"&start=\" +str(num)\n","        r = requests.get(url_current_page)\n","        soup = BeautifulSoup(r.content, 'lxml')\n","        \n","        # Monitor requests and status of requests\n","        req +=1 \n","        #t = time() - start_time  \n","        if r.status_code != 200:  \n","            print('Request: {} | Status code: {}'.format(req, r.status_code))\n","            break \n","\n","        # Get list of url's of jobs\n","        job_urls = get_job_urls(soup, base_url='https://www.indeed.com')\n","        jobs_extracted += len(job_urls)\n","        \n","        # For each url-job, extract raw html and append to list of raw htmls\n","        for url in job_urls: \n","            job = extract_job_information(url)\n","            data.append(job)\n","   \n","        # Get total requests and total job url's retrieved\n","        print(\"Request: {} | Total jobs extracted: {}\".format(req, jobs_extracted))\n","        clear_output(wait = True) \n","\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcqaWHF3CgoV","colab_type":"text"},"source":["# 3) Scrape data"]},{"cell_type":"code","metadata":{"id":"wNGJ-l4qCgoW","colab_type":"code","outputId":"6cb6993d-9198-4b0d-99e6-e7aacd8bce09","colab":{}},"source":["#Implement this to scrape data \n","\n","data = job_scraper(\"data scientist\", \"washington dc\")\n","#data = job_scraper(\"software engineer\", \"washington dc\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Request: 1 | Total jobs extracted: 15\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lz18pN46Cgoa","colab_type":"code","outputId":"15186dd9-5359-4884-a6a0-6163bb5128b7","colab":{}},"source":["data[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'company': 'Guzman & Griffin Technologies Inc',\n"," 'description': \"Please Apply using this link: https://app.smartsheet.com/b/form/2cb8018ed6a041b0870e3cd056c286abOur FocusIn aviation safety, we seek to minimize the potential for harm to the flying public. To support thiseffort, we collect vast amounts of operational and simulation data. How can we use statisticalanalyses and data science to analyze this data and effect meaningful change to aviation safety? Ifthis sounds interesting to you, GGTI seeks a Data Scientist to join its Data Science & Analytics Team.What’s the Job?· Preprocessing, cleansing, and verifying integrity of data that can be provided as input foradvanced analytics· Ad-hoc analysis and presentations of results for technical and non-technical audiences· Feature selection, model building and optimization using machine learning techniques· Building regression, classification, association models· Building anomaly detection systemsAbout YouYou are an experienced Data Scientist with an advanced degree in a quantitative discipline. You will be able to discover information hidden in vast amounts of aviation data, including operation and simulated data, and provide recommendations to managements at various levels. Along with working with air traffic domain experts, you will be able to apply data mining techniques, perform statistical analyses, and build high quality predictive models and causal analyses of safety issues. You can analyze large, complex, multi-dimensional datasets with R and/or Python and Business Intelligence (BI) tools such as Tableau. In addition, you have some familiarity with SQL.What do you need to qualify?*· 10+ years of professional work experience in a quantitative field· A Ph.D. or Master’s Degree in operations research, applied statistics, Computer Science (data mining, machine learning stream) or a related quantitative discipline· Deep understanding of statistical and predictive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms· Excellent applied statistics skills, such as distributions, statistical testing, regression, etc.· Excellent understanding of common machine learning techniques and algorithms, such as K-means, k-NN, Bayesian Networks, SVM, Decision trees, ensemble methods etc.· Experience with common data science toolkits, such as Python, R, Weka, NumPy, MatLab, etc. Proficiency in at least one of these (preferably Python)· Experience with data visualization tools, such as D3.js, Tableau etc.· Some experience with NoSQL databases, such as MongoDB, Cassandra, or HBase· Ability to pass a Federal Government background investigation to obtain a Public Trust clearance.· Current U.S. Citizen/Green Card-holderStand-out SkillsDeep learning with TensorFlow, Torch, or similarSpark, Hadoop, or similarSQL, Java or C/C++.Please Apply using this link: https://app.smartsheet.com/b/form/2cb8018ed6a041b0870e3cd056c286abJob Type: Full-timeSalary: $110,000.00 to $145,000.00 /yearExperience:Data Science: 10 years (Preferred)Education:Master's (Required)Work authorization:United States (Required)Work Location:One locationBenefits:Health insuranceDental insuranceVision insuranceRetirement planPaid time offWork from homeFlexible scheduleProfessional development assistanceTuition reimbursementSchedule:Monday to FridayCompany's website:https://ggti.com/Benefit Conditions:Only full-time employees eligibleWork Remotely:Temporarily due to COVID-19\",\n"," 'position': 'Experienced Data Scientist',\n"," 'sector': None,\n"," 'url': 'https://www.indeed.com/company/Guzman-&-Griffin-Technologies,-Inc/jobs/Experienced-Data-Scientist-0337222ee0204e5b?fccid=4a43896aa6598a77&vjs=3'}"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"Tb9ZRg2SCgod","colab_type":"text"},"source":["# 4) Store data in json file"]},{"cell_type":"code","metadata":{"id":"afQ0hz8tCgoe","colab_type":"code","colab":{}},"source":["def store_data(filename, dictionary):\n","    import json\n","    with open(filename, 'w') as f:\n","        file = json.dump(dictionary, f, indent=4)\n","    return file"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IgKZCSqOCgoh","colab_type":"code","colab":{}},"source":["#Implement this to store json file in working directory \n","\n","#store_data('dc_datascience.json', data) \n","#store_data('dc_software-engineer.json', data)"],"execution_count":0,"outputs":[]}]}