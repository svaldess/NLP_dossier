{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Natural Language Processing\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Entity extraction with hand annotation and SpaCy\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Author: Sandra Valdés Salas\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "April 2020\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Table of Contents_\n",
    "* [Part 1: Compare hand annotations](#first)\n",
    "    * [1) Build model](#first1.1)\n",
    "    * [2) Compare files](#first1.2)\n",
    "    * [3) Calculate Cohen's Kappa](#first1.3)\n",
    "    * [4) Results](#first1.4)\n",
    "\n",
    "* [Part 2: Compare entities extracted by 2 models](#second)\n",
    "    * [1) Build method to compare entities from dataset of news](#second1.1)\n",
    "    * [2) Extract entities with Model 1 as reference and Model 2 as test](#second1.2)\n",
    "    * [3) Recall and Precision](#second1.3)\n",
    "    * [4) Results](#second1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Introduction_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important task in Natural Language Processing is the identification of entities (person, locations, organizations, geopolitical entities, etc.) in a document. Entities can be hand annotated with tools such as [Dataturks.com](https://dataturks.com/). However, this can be problematic due to discrepancies between the hand annotators. \n",
    "\n",
    "The first part of this exercise compares the annotations between two annotators that tagged entities in 5 documents. Agreement between the 2 annotators is calculated with Cohen Kappa's coefficient.\n",
    "\n",
    "The second part of this exercise compares the entities extracted from a corpus of news articles. Entities were extracted with two SpaCy models. By considering the model 1 (\"en_core_web_sm\") as a reference model and the model 2 (\"en_core_web_md\") as a test model, the folllowing entities are identified:\n",
    "\n",
    "- Identified: entities that are in reference and test models \n",
    "- Unidentified: entities that are in the reference model but not in test model \n",
    "- Spurious: entities that are in test model but not in reference model\n",
    "\n",
    "Finally, recall and precision are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Import libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# method to read annotation file\n",
    "def annotation_processor(annotation_file):\n",
    "    annotation_array = []\n",
    "\n",
    "    # here we need to be careful and process each line of the annotation file separately\n",
    "    read_annotation = open(annotation_file)\n",
    "    for line in read_annotation:\n",
    "        data = json.loads(line)\n",
    "        annotation_array.append(data)\n",
    "\n",
    "    # here we return an array of the individual annotations\n",
    "    return annotation_array\n",
    "    \n",
    "# calling the annotation processor function\n",
    "#annotation_processor('./annotated_data/annotated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will create two objects to store the reference annotations and your own annotations\n",
    "annotations_array_person_A = annotation_processor('./annotated_data/annotated.json')\n",
    "\n",
    "# here I am just putting the same file in... if I do this I would expect a perfect match\n",
    "annotations_array_person_B = annotation_processor('./annotated_data/my_annotations.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each array is a dictionary with annotation, content, extras, metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation': [{'label': ['Person'],\n",
       "   'points': [{'end': 386, 'start': 379, 'text': 'Blackmon'}]},\n",
       "  {'label': ['Person'],\n",
       "   'points': [{'end': 296, 'start': 293, 'text': 'Irma'}]},\n",
       "  {'label': ['Person'],\n",
       "   'points': [{'end': 272, 'start': 267, 'text': 'Harvey'}]},\n",
       "  {'label': ['Person'],\n",
       "   'points': [{'end': 79, 'start': 74, 'text': 'Hardin'}]},\n",
       "  {'label': ['Person'],\n",
       "   'points': [{'end': 71, 'start': 64, 'text': 'Blackmon'}]},\n",
       "  {'label': ['Person'],\n",
       "   'points': [{'end': 71,\n",
       "     'start': 42,\n",
       "     'text': 'Chief Deputy Jonathan Blackmon'}]}],\n",
       " 'content': 'According to Polk County Sheriff’s Office Chief Deputy Jonathan Blackmon, Hardin is a Rome native who worked with RomeCares and Floyd Sheriff’s officials to get the supplies to a disaster relief zone.\\n\\n“We originally took in the supplies for the victims of Hurricane Harvey in Texas, but when Irma hit closer to home we felt it would be more beneficial to send them to Florida,” Blackmon said.\\n\\nThe team that went to Florida with the supplies helped with relief efforts in the Moore Haven area as well.',\n",
       " 'extras': None,\n",
       " 'metadata': {'evaluation': 'NONE',\n",
       "  'first_done_at': 1541085438000,\n",
       "  'last_updated_at': 1541085438000,\n",
       "  'last_updated_by': 'nJUvh8eg6cTQb2imeaFWqJDvwDt1',\n",
       "  'sec_taken': 0,\n",
       "  'status': 'done'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SAMPLE FROM REFERENCE ANNOTATION\n",
    "one_annotation_from_array = annotations_array_person_A[0]\n",
    "one_annotation_from_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Compare hand annotations <a class=\"anchor\" id=\"first\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Build method <a class=\"anchor\" id=\"first1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Function to extract points from annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to extract relevant information from the annotations\n",
    "\n",
    "def points_extractor(one_annotation_from_array):\n",
    "    \"\"\"\n",
    "    This function evaluates the annotations of one document.\n",
    "    Returns: a list of dicionaries with end points, start points and text of the entity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract values from annotation\n",
    "    annotation_information = {key:value for (key,value) in one_annotation_from_array.items() if key=='annotation'}\n",
    "\n",
    "    points_list = []\n",
    "    \n",
    "    #extract the points if the document has a \"person\" tagged\n",
    "    try:\n",
    "        # extract information that matches the key = \"annotation\"\n",
    "        for key, value in annotation_information.items():\n",
    "            # iterate items in values of the dictionary\n",
    "            for items in value:\n",
    "                # Get points and append to points:list\n",
    "                for item in items.get('points'):\n",
    "                    points_list.append(item)\n",
    "                            \n",
    "    # Return \"empty list\" in case the document has no \"person\" tagged\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    return points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end': 460, 'start': 453, 'text': 'Morrison'},\n",
       " {'end': 325, 'start': 315, 'text': 'the leaders'},\n",
       " {'end': 91, 'start': 80, 'text': 'eter O’Neill'},\n",
       " {'end': 39, 'start': 32, 'text': 'Morrison'},\n",
       " {'end': 39, 'start': 26, 'text': 'Scott Morrison'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from annotations reference\n",
    "one_annotation_person_A = annotations_array_person_A[1]\n",
    "annotations_points_person_A = points_extractor(one_annotation_person_A)\n",
    "annotations_points_person_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end': 460, 'start': 453, 'text': 'Morrison'},\n",
       " {'end': 325, 'start': 315, 'text': 'the leaders'},\n",
       " {'end': 91, 'start': 79, 'text': 'Peter O’Neill'},\n",
       " {'end': 39, 'start': 32, 'text': 'Morrison'},\n",
       " {'end': 39, 'start': 26, 'text': 'Scott Morrison'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from my annotations\n",
    "one_annotation_person_B = annotations_array_person_B[1]\n",
    "annotations_points_person_B = points_extractor(one_annotation_person_B)\n",
    "annotations_points_person_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Method to count matches between my annotations and reference annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **points_extractor function** returns a list of dictionaries with the start, end and text information for each entity tagged. By converting this list into a set of tuples, we can compare \"my annotations\" with \"reference annotations\" by taking a look at their intersection and symmetric difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits: 4\n",
      "----\n",
      "Hits: {(('start', 315), ('end', 325), ('text', 'the leaders')), (('start', 32), ('end', 39), ('text', 'Morrison')), (('start', 26), ('end', 39), ('text', 'Scott Morrison')), (('start', 453), ('end', 460), ('text', 'Morrison'))}\n",
      "\n",
      "\n",
      "Total missed: 2\n",
      "----\n",
      "Missed: {(('start', 79), ('end', 91), ('text', 'Peter O’Neill')), (('start', 80), ('end', 91), ('text', 'eter O’Neill'))}\n"
     ]
    }
   ],
   "source": [
    "#Compare start\" & \"end\" reference annotations and my annotations \n",
    "# Get a set of tuples for each annotation\n",
    "set_A = set(tuple(d.items()) for d in annotations_points_person_A)\n",
    "set_B = set(tuple(d.items()) for d in annotations_points_person_B)\n",
    "\n",
    "#Look at common tuples\n",
    "hits = set_A.intersection(set_B)\n",
    "print('Total hits: {}'.format(len(hits)))\n",
    "print('----')\n",
    "print('Hits: {}'.format(hits))\n",
    "print('\\n')\n",
    "\n",
    "#Look at uncommon tuples\n",
    "missed = set_A.symmetric_difference(set_B)\n",
    "print('Total missed: {}'.format(len(missed)))\n",
    "print('----')\n",
    "print('Missed: {}'.format(missed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Putting everything together... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compare tuples and get number of hits and misses\n",
    "\n",
    "def compare_annotations(annotations_A, annotations_B):\n",
    "    \"\"\"\n",
    "    This function compares two different annotations for the same document. \n",
    "    Returns: categories matches, non_matches, partial_matches. \n",
    "    Each category contains a list of dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Extract 'points' using points_extractor function\n",
    "    #The result is a list of dictionaries with the keys ->'end', 'start', 'text' \n",
    "    annotations_points_person_A = points_extractor(annotations_A)\n",
    "    annotations_points_person_B = points_extractor(annotations_B)\n",
    "    \n",
    "    #Convert the above results into a set of tuples\n",
    "    set_A = set(tuple(d.items()) for d in annotations_points_person_A)\n",
    "    set_B = set(tuple(d.items()) for d in annotations_points_person_B)\n",
    "    \n",
    "    # calculate hits -> intersection \n",
    "    intersection = set_A.intersection(set_B)\n",
    "    # calculate missed -> symmetric difference \n",
    "    symm_difference = set_A.symmetric_difference(set_B)\n",
    "    \n",
    "    #re convert set of tuples to dictionary \n",
    "    final_hits = [dict(tuple) for tuple in list(intersection)]\n",
    "    missed = [dict(tuple) for tuple in list(symm_difference)]\n",
    "    \n",
    "    #--------BONUS---------\n",
    "    # Calculate partial matches\n",
    "    partial = []\n",
    "    index = 0\n",
    "    while index < len(missed)-1:\n",
    "        # Check if \"start\" matches\n",
    "        if missed[index]['start']==missed[index+1]['start']:\n",
    "            partial.append(missed[index])\n",
    "        # Check if \"end\" matches\n",
    "        elif missed[index]['end']==missed[index+1]['end']:\n",
    "            partial.append(missed[index])\n",
    "        index+=1\n",
    "    #-----END OF BONUS------\n",
    "    \n",
    "    # Delete missed info that appears in partial info\n",
    "    final_missed = []\n",
    "    final_partial = []\n",
    "    for dict_ in missed:\n",
    "        if dict_ in partial:\n",
    "            final_partial.append(dict_)\n",
    "        else:\n",
    "            final_missed.append(dict_)\n",
    "    \n",
    "    return final_hits, final_missed, final_partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compare files   <a class=\"anchor\" id=\"first1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_annotation_files(array_A, array_B):\n",
    "    \"\"\"\n",
    "    This function compares two different annotations across many documents. \n",
    "    Returns: \n",
    "    the number of matches, number of non mathces and number of partial matches\n",
    "    between the two arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Keep record of entities by category\n",
    "    all_matches = []\n",
    "    all_non_matches = []\n",
    "    all_partial_matches = []\n",
    "    \n",
    "    #Keep record of number of entities by category \n",
    "    num_matches = 0\n",
    "    num_non_matches = 0\n",
    "    num_partial_matches = 0\n",
    "\n",
    "    # for each annotation in the reference_annotations (person A)\n",
    "    for annotation in array_A:\n",
    "        # for each annotation in my_annotations (person B)\n",
    "        for other_annotation in array_B:\n",
    "            # Match content from both arrays\n",
    "            if (annotation[\"content\"] == other_annotation[\"content\"]):\n",
    "\n",
    "                # Compare annotations \n",
    "                matches, non_matches, partial_matches = compare_annotations(annotation, other_annotation)\n",
    "                \n",
    "                # Append results to lists of entities by category\n",
    "                all_matches.append(matches)\n",
    "                all_non_matches.append(non_matches)\n",
    "                all_partial_matches.append(partial_matches) \n",
    "                \n",
    "                # Calculate numbers\n",
    "                num_matches += len(matches)\n",
    "                num_non_matches += len(non_matches)\n",
    "                num_partial_matches += len(partial_matches)\n",
    "\n",
    "    print('Success!!')\n",
    "    print('*************')\n",
    "    print('Hits: ', num_matches)\n",
    "    print('Missed: ', num_non_matches)\n",
    "    print('Partial hits: ', num_partial_matches)\n",
    "    \n",
    "    return all_matches, all_non_matches, all_partial_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!!\n",
      "*************\n",
      "Hits:  21\n",
      "Missed:  11\n",
      "Partial hits:  2\n"
     ]
    }
   ],
   "source": [
    "hits, missed, partial = compare_annotation_files(annotations_array_person_A, annotations_array_person_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Calculate Cohen's Kappa  <a class=\"anchor\" id=\"first1.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Cohen Kappa score with a chance of agreement of 0.3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed agreement:  0.5\n"
     ]
    }
   ],
   "source": [
    "observed_agreement = (len(hits) / (len(hits)+len(missed)))\n",
    "print('Observed agreement: ', observed_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa score with 0.3 chance of agreement is: 0.2857\n"
     ]
    }
   ],
   "source": [
    "chance = 0.3\n",
    "kappa = (observed_agreement-chance)/(1-chance)\n",
    "print('Kappa score with {} chance of agreement is: {}'.format(chance, round(kappa,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Results   <a class=\"anchor\" id=\"first1.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Identifying a \"Person\" is difficult. By comparing both annotations, we can identify the following types mistakes:\n",
    "- **1) Tagging the start or end of the entity incorrectly.** For instance, in the reference annotations we can find a comma at the end of \"Piper Merrit,\" or a missing \"P\" at the beginning of \"eter O'Neill\". In this case, it is useful to identify partial matchings. \n",
    "- **2) Tagging (or not tagging) a combination of words associated to the entity.** For instance, I did not include _Chief Deputy_ when tagging \"Jonathan Blackmon\". In this case, clear rules for hand annotation are useful for avoiding this type of events. \n",
    "- **3) Tagging unambiguous words**. For instance, considering _'A group of students and teachers'_ as a Person would depend on the annotator. \n",
    "- **4) Tagging names that refer to other entities (events, location, organization).** For instance, in the reference annotations, hurricanes \"Harvey\" and \"Irma\", as well as \"George Washington\" (which in the document refered to a location), were incorrectly tagged as Person. \n",
    "\n",
    "The Kappa Score between the reference annotations and my annotations was 0.2857. If we consider that the chance of agreement is 0.3, then we can conclude that this score is high (even though perfect agreement is 1). The chance of agreement, however, is relative. Thus the Cohen Kappa score could not be particular useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [{'end': 91, 'start': 79, 'text': 'Peter O’Neill'}],\n",
       " [],\n",
       " [{'end': 231, 'start': 218, 'text': 'Piper Merritt,'}],\n",
       " []]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partially matched entities in document 2 and 4\n",
    "partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'end': 386, 'start': 379, 'text': 'Blackmon'},\n",
       "  {'end': 71, 'start': 64, 'text': 'Blackmon'}],\n",
       " [{'end': 325, 'start': 315, 'text': 'the leaders'},\n",
       "  {'end': 39, 'start': 32, 'text': 'Morrison'},\n",
       "  {'end': 39, 'start': 26, 'text': 'Scott Morrison'},\n",
       "  {'end': 460, 'start': 453, 'text': 'Morrison'}],\n",
       " [{'end': 10, 'start': 0, 'text': 'Joe Montana'},\n",
       "  {'end': 275, 'start': 269, 'text': 'Montana'},\n",
       "  {'end': 10, 'start': 4, 'text': 'Montana'},\n",
       "  {'end': 44, 'start': 33, 'text': 'Dwight Clark'},\n",
       "  {'end': 259, 'start': 245, 'text': 'the quarterback'}],\n",
       " [{'end': 537, 'start': 532, 'text': 'Walker'},\n",
       "  {'end': 710, 'start': 701, 'text': 'John Deere'},\n",
       "  {'end': 825, 'start': 816, 'text': 'John Deere'},\n",
       "  {'end': 34, 'start': 25, 'text': 'John Deere'},\n",
       "  {'end': 510, 'start': 501, 'text': 'John Deere'},\n",
       "  {'end': 594, 'start': 588, 'text': 'Merritt'},\n",
       "  {'end': 230, 'start': 224, 'text': 'Merritt'},\n",
       "  {'end': 581, 'start': 576, 'text': 'Walker'},\n",
       "  {'end': 537, 'start': 527, 'text': 'Doug Walker'},\n",
       "  {'end': 192, 'start': 183, 'text': 'John Deere'}],\n",
       " []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Succesfully matched entities in documents 1 to 4\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'end': 79, 'start': 74, 'text': 'Hardin'},\n",
       "  {'end': 272, 'start': 267, 'text': 'Harvey'},\n",
       "  {'end': 71, 'start': 42, 'text': 'Chief Deputy Jonathan Blackmon'},\n",
       "  {'end': 296, 'start': 293, 'text': 'Irma'},\n",
       "  {'end': 71, 'start': 55, 'text': 'Jonathan Blackmon'}],\n",
       " [{'end': 91, 'start': 80, 'text': 'eter O’Neill'}],\n",
       " [{'end': 420, 'start': 416, 'text': 'Clark'},\n",
       "  {'end': 44, 'start': 40, 'text': 'Clark'}],\n",
       " [{'end': 230, 'start': 218, 'text': 'Piper Merritt'}],\n",
       " [{'end': 116, 'start': 85, 'text': 'A group of students and teachers'},\n",
       "  {'end': 60, 'start': 44, 'text': 'George Washington'}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mistakenly identified entities in all documents\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Compare entities extracted by 2 models <a class=\"anchor\" id=\"second\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Build method to compare entities from dataset of news <a class=\"anchor\" id=\"second1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "model_1 = spacy.load(\"en_core_web_sm\")\n",
    "model_2 = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = \"./news_data/\"\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename , encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(join(directory, f))\n",
    "        print(file_text)\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts\n",
    "    \n",
    "# UNCOMMENT THIS LINE TO GENERATE THE RESULTS OF PART 2:\n",
    "\n",
    "#text_corpus = read_directory_files(dir_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Method to extract entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entities\n",
    "def get_entities(document_text, model):\n",
    "    '''\n",
    "    This function returns entities from a document tagged by a specific spacy model\n",
    "    '''\n",
    "    analyzed_doc = model(document_text)\n",
    "    return [entity for entity in analyzed_doc.ents if entity.label_ in [\"PERSON\", \"ORG\", \"LOC\", \"GPE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Method to compare entities extracted by 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entities_from_document(reference_entities, test_entities):\n",
    "    '''\n",
    "    This function compares the entities identified by a reference model and a test model. \n",
    "    The function returns the following categories:\n",
    "    - Identified: entities that are in reference and test models \n",
    "    - Unidentified: entities that are in the reference model but not in test model \n",
    "    - Spurious: entities that are in test model but not in reference model\n",
    "    '''\n",
    "    \n",
    "    correct_identified_entities = []\n",
    "    correct_unidentified_entities = []\n",
    "    spurious_identified_entites = []\n",
    "    \n",
    "    # items in the test set that are also in the reference set\n",
    "    for ent_test in test_entities:\n",
    "        for ent_ref in reference_entities:\n",
    "            # if the text and label are equal, append to identified list\n",
    "            if (ent_test.text==ent_ref.text) and (ent_test.label_==ent_ref.label_):\n",
    "                correct_identified_entities.append(ent_test)\n",
    "            # if the above condition is not met, append to unidentified list\n",
    "            elif (ent_test.text==ent_ref.text) and (ent_test.label_!=ent_ref.label_):\n",
    "                correct_unidentified_entities.append(ent_test)\n",
    "    \n",
    "    # items in the reference set and that are NOT IN THE TEST SET \n",
    "    for ent_ref in reference_entities:\n",
    "        if ent_ref not in test_entities:\n",
    "            correct_unidentified_entities.append(ent_ref)\n",
    "    \n",
    "    # items in the test set that are NOT IN THE REFERENCE SET\n",
    "    for ent_test in test_entities:\n",
    "        if ent_test not in reference_entities and ent_test not in correct_identified_entities:\n",
    "            spurious_identified_entites.append(ent_test)\n",
    "    \n",
    "    return correct_identified_entities, correct_unidentified_entities, spurious_identified_entites\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Extract entities with Model 1 as reference and Model 2 as test <a class=\"anchor\" id=\"second1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories list\n",
    "overall_identified_entities = []\n",
    "overall_unidentified_entities = []\n",
    "overall_spurious_entities = []\n",
    "\n",
    "for document in text_corpus:\n",
    "    \n",
    "    # *******Set model reference and model test*******\n",
    "    entities_1 = get_entities(document[\"content\"], model_1)\n",
    "    entities_2 = get_entities(document[\"content\"], model_2)\n",
    "    # ***********************************************\n",
    "    \n",
    "    # Apply function to identify entites by category\n",
    "    identified, unidentified, spurious = compare_entities_from_document(entities_1, entities_2)\n",
    "    \n",
    "    # Append list of entities extracted in every document to main lists\n",
    "    overall_identified_entities.append(identified)\n",
    "    overall_unidentified_entities.append(unidentified)\n",
    "    overall_spurious_entities.append(spurious)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list of lists (for each category). The later contains the entities extracted from every document.\n",
    "For example, below is the first list of \"correctly identified entities\" from document 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a function to flatten the list of lists and to manipulate the date more easily.\n",
    "def flatten_list(list_):\n",
    "    return [ent for entity in list_ for ent in entity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to model 1 as reference and model 2 as test\n",
    "identified_entities = flatten_list(overall_identified_entities)\n",
    "unidentified_entities = flatten_list(overall_unidentified_entities)\n",
    "spurious_entites = flatten_list(overall_spurious_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Recall and Precision <a class=\"anchor\" id=\"second1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_identified = len(identified_entities)\n",
    "num_unidentified = len(unidentified_entities)\n",
    "num_spurious = len(spurious_entites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9576291942199363\n"
     ]
    }
   ],
   "source": [
    "#How useful or relevant?\n",
    "precision = num_identified / (num_identified + num_spurious)\n",
    "print('Precision: ',precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.8771733034212003\n"
     ]
    }
   ],
   "source": [
    "#How complete?\n",
    "recall = num_identified / (num_identified + num_unidentified)\n",
    "print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Results <a class=\"anchor\" id=\"second1.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When testing model 2 (our reference is model 1) we can see that the **_precision is higher than recall_**. \n",
    "- **Precision**, measures the total hits (correctly idendified entities) divided by all retrieved entities (relevant and not relevant). In other words, it refers to the percentage of the results which are relevant and, since it is high, we can speculate that this is a better model.\n",
    "- **Recall**, on the other hand, measures the total hits (correctly idendified entities) divided by all relevant entities (retrieved and not retrieved); in other words, it refers to the percentage of total relevant results correctly classified by the test model.\n",
    "- A large number of entities unidentified by model 2 were from the type \"PERSON\" (506 in total), followed by \"ORG\" (436 in total). This suggests that, if we are more interested in extracting \"PERSON\" and \"ORG\" accurately, model 1 would be a better model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of entities in each category with Model 1 as reference and Model 2 as test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct identified entities:  7820\n",
      "Correct unidentified entities:  1095\n",
      "Spurious entities identified:   346\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print('Correct identified entities: ', num_identified)\n",
    "print('Correct unidentified entities: ', num_unidentified)\n",
    "print('Spurious entities identified:  ', num_spurious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number and type of unidentified entities by Model 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count entities \n",
    "def get_frequencies(list_):\n",
    "    dict_ = {}\n",
    "    for entity in list_:\n",
    "        if entity not in dict_.keys():\n",
    "            dict_[entity] = 1\n",
    "        else:\n",
    "            dict_[entity]+=1\n",
    "            \n",
    "    dict_sorted = {k: v for k, v in sorted(dict_.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    return dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Yukos': 50, 'Rover': 43, 'Kilroy-Silk': 36, 'Mini': 32, 'Tory': 32, 'Houston': 30, 'Yugansk': 28, 'Commons': 25, 'Hague': 25, 'Umbro': 21, 'Mr Blair': 19, 'Blair': 19, 'Wales': 18, 'Maserati': 18, 'Multiplex': 16, 'Changyu': 16, 'Ms Diemont': 16, 'Mr Brown': 15, 'Rosneft': 13, 'MPs': 12, 'Wembley': 12, 'BA': 11, 'Agroflora': 10, 'the Liberal Democrats': 9, 'Mr Blunkett': 9, 'Ms Hutt': 9, 'Mr Mubanga': 8, 'Mr Straw': 8, 'Lib Dem': 7, 'Mr Kilroy-Silk': 7, 'Harry': 7, 'Ryanair': 7, 'Parmalat': 6, 'Welsh': 6, 'Lords': 6, 'Falconer': 6, 'Tods Murray': 6, 'Mr Clarke': 6, 'ICT': 6, 'McConnell': 6, 'Marsh': 5, \"Mr Brown's\": 5, 'Veritas': 5, 'Parker Bowles': 5, 'Pernod': 5, 'G7': 4, 'Kilroy': 4, 'Illva Saronno': 4, 'West Ham': 4, 'Howard': 4, 'Gazprom': 4, 'Mr Baume': 4, 'Highlands': 4, 'Sepa': 4, 'Gibbons': 4, 'RAF Fairford': 4, 'Guantanamo': 3, 'Tories': 3, 'Labour': 3, 'Cowley': 3, 'Lib Dems': 3, 'CSA': 3, \"Mr Blunkett's\": 3, 'Mr Pound': 3, 'Mr Hockney': 3, 'Diana': 3, 'NAO': 3, 'Lamberty': 3, 'Rank': 3, 'Mittal': 3, 'Fox': 3, 'Learmount': 3, 'Prince Harry': 3, 'Mr Howard': 3, 'Mr McConnell': 3, 'Perrier': 3, 'Asbos': 3, 'Spitzer': 2, \"Mr Mubanga's\": 2, 'Foster': 2, 'Ms Sturgeon': 2, 'McLetchie': 2, 'Kent': 2, \"New Labour's\": 2, 'Kashmiris': 2, 'the Line of Control': 2, 'Islamophobia': 2, 'Mrs Parker Bowles': 2, 'Clarence House': 2, 'Britons': 2, 'Highfield': 2, 'Durham': 2, 'Telegraph': 2, 'Election Court': 2, 'Illva': 2, 'Doubleclick': 2, 'MEP': 2, 'Guy Mansfield QC': 2, 'Galloway': 2, 'Amicus': 2, 'Belmarsh': 2, 'Scotland': 2, 'Cafod': 2, 'Oxfam': 2, 'Plaid Cymru': 2, 'Mr Houston': 2, 'Lord Scarman': 2, 'Alfa Romeo': 2, 'Mr Younger': 2, 'Younger': 2, 'Mr Bewlay': 1, 'Green MSP': 1, 'Menzies Campbell': 1, 'Observer': 1, \"the Liberal Democrats'\": 1, \"BBC One's\": 1, \"the Lib Dem's\": 1, 'Torfaen': 1, 'Finance': 1, 'Mr Wellington': 1, 'Ms Dunwoody': 1, 'University College': 1, 'MG Rover': 1, 'Mini One': 1, 'Longbridge': 1, 'Budget': 1, 'Parul Jain': 1, 'Frost': 1, 'CS2': 1, \"CSA '\": 1, 'Pensions': 1, 'White City': 1, 'Wembley Stadium': 1, 'Nimrod': 1, 'Mr McLetchie': 1, \"Holyrood's Chamber Office\": 1, 'Labour Party': 1, 'John Redwood': 1, 'central bank': 1, 'DZ bank': 1, 'Cabinet if Labour': 1, 'lover - Kimberly Quinn -': 1, \"Alan Budd's\": 1, 'the Ealing North MP': 1, 'rang Lucy': 1, \"Mr Marr's\": 1, 'Sub-Saharan Africa': 1, 'SNP Holyrood': 1, \"the Quarterly Economic Commentary of Strathclyde University's\": 1, 'Arthur Andersen': 1, 'Mr Sullivan': 1, 'Asahi Breweries': 1, 'Shinnama': 1, 'Srinagar': 1, 'Kashmiri': 1, 'East Midlands': 1, 'Ex-BBC': 1, 'Eurosceptic party': 1, 'Europe': 1, 'UK': 1, 'Figures': 1, 'Camilla Parker Bowles': 1, \"St George's\": 1, 'Ros Coward': 1, 'HRH Duchess of Cornwall': 1, 'Queen': 1, 'Prince Charles': 1, 'the Archbishop of Canterbury': 1, 'Charles': 1, 'Bowles': 1, \"The Prince's Trust\": 1, 'Mr Kemp': 1, 'Britain': 1, 'the European Anti-Fraud Office': 1, 'Best Buy': 1, 'Mr Armstrong': 1, 'Frankfurt': 1, 'Rhein-Main': 1, 'Spennymoor': 1, 'Zanussi': 1, 'Frigidaire': 1, '12bn euros': 1, 'Mr Bondi': 1, 'Mr Greenspan': 1, 'Barclay': 1, 'Mr Fitzpatrick': 1, 'Guardian': 1, 'David -': 1, \"Conrad Black's\": 1, 'The Sunday Telegraph': 1, 'non-Opec': 1, 'Islam': 1, 'house': 1, 'Cojedes': 1, 'New EU': 1, 'Heineken': 1, 'Scottish & Newcastle': 1, 'Yantai': 1, 'Ask Jeeves': 1, 'Glenmorangie': 1, 'Stolichnaya': 1, 'Dunkin': 1, 'Domecq\\n\\n': 1, 'Pernod Ricard': 1, 'GMT': 1, \"Pernod's\": 1, 'Havana Club rum': 1, 'Baskin-Robbins': 1, 'euros': 1, 'Eddington': 1, 'Dresdner Kleinwort Wasserstein': 1, 'Aviation': 1, 'Nick Van': 1, 'Lorillard Tobacco': 1, 'Altria': 1, 'the Court of Appeals for the District of Columbia': 1, 'State': 1, 'Hypovereinsbank': 1, 'the Sarbanes-Oxley Act': 1, 'Lucas': 1, 'Dewsbury': 1, 'Mr Malik': 1, 'Bernie Grant -': 1, 'Nagano': 1, 'Grey Global': 1, 'Omnicom': 1, 'Reuters': 1, 'Ogilvy & Mather and Cordiant Communications': 1, 'US Congressional': 1, 'Pinewood': 1, 'Deluxe Media': 1, 'Leisure': 1, 'Deluxe Films': 1, 'VHS': 1, 'Early Warning System': 1, 'The Lib Dems': 1, 'Mr Yeo': 1, 'Mr Davis': 1, 'Cabinet': 1, 'ID': 1, 'Hutus': 1, 'the Survivors Fund': 1, 'SURF': 1, 'G20': 1, \"MG Rover's\": 1, 'Transport & General': 1, 'Grantham': 1, 'Dafur': 1, 'the Parliamentary committee': 1, 'Ex-Labour': 1, 'Al-Jazeera': 1, 'Mr Galloway': 1, 'the British National Party(BNP)': 1, \"Mr Betts-Green's\": 1, 'Mr Khodorkovsky -': 1, 'Communications': 1, 'Mr Maran': 1, 'Sterling': 1, 'Lakshmi Mittal': 1, 'Mr Mittal': 1, 'Somerset': 1, 'Home': 1, 'Sellafield': 1, 'Kodiak': 1, 'Sierra': 1, 'Yukon': 1, 'Chevrolet': 1, 'Avalanche': 1, 'Express': 1, 'the GMC Savana': 1, 'Pontiac Grand Prix': 1, 'Chevrolet Trailblazers': 1, 'Isuzu Ascenders': 1, 'Shadow': 1, 'EU': 1, 'the European Union': 1, 'the EU Common Agricultural Policy': 1, 'the Sunday Times': 1, 'Whitehall - Labour': 1, 'Oliver Heald': 1, 'Mr Manuel': 1, 'Dreamliner': 1, 'International Development': 1, 'RAF': 1, 'HMS Chatham': 1, 'Asia': 1, 'BBC News': 1, 'Hospital': 1, 'Welsh Health': 1, 'Unison': 1, 'Super': 1, 'General Workers Union': 1, 'Auschwitz': 1, 'Janner': 1, 'The Beaufort Hunt': 1, 'Labour MP': 1, 'Commerce': 1, 'Vancheshwar': 1, 'hotels': 1, 'Mr Nath': 1, 'Tamil Nadu': 1, 'England': 1, 'replica England': 1, 'Law Lords': 1, 'Mr Hague': 1, \"Mr Baume's\": 1, 'Rolls-Royce -': 1, 'Derbyshire': 1, 'Yorkshire': 1, 'Mr Spindler': 1, 'UK Coal': 1, 'the West Midlands': 1, 'Lisa Power': 1, 'Theresa May': 1, 'Suffolk': 1, 'Kent Hunt': 1, 'the League Against Cruel Sports -': 1, 'Lord Goldsmith': 1, 'Blackberry': 1, 'Alastair Campbell': 1, 'ONS': 1, 'GCSE': 1, 'Hebrides': 1, 'Climate': 1, 'the University of the Highlands and Islands': 1, 'UHI': 1, 'Millennium Institute': 1, 'Commercial Services union': 1, 'Mr Barber': 1, 'PCS': 1, 'Price': 1, 'Band': 1, 'Mr Duncan': 1, 'Blackpool': 1, 'State for Culture, Media and Sport': 1, 'the Gambling Bill': 1, \"Age Concern's\": 1, 'The Times': 1, 'Mr Allawi': 1, 'the Ministry of Economy, Trade and Industry (METI': 1, 'the Nikkei down': 1, 'Enel': 1, '6.5bn euros': 1, 'Mr Lewis': 1, 'Potato Council': 1, 'Commission for': 1, 'Mr Kennedy': 1, 'the Medical Foundation for the Care of Victims of Torture': 1, 'Dai-ichi Life Research Institute': 1, 'Blaengwynfi': 1, \"Ms Hutt's\": 1, 'Ieuan Wyn Jones': 1, 'UKIP': 1, 'Herald': 1, 'UKIP Scotland': 1, 'Fairford': 1, 'Glos': 1, 'Gloucestershire Police': 1, 'Mr Justice Harrison': 1, 'Justices Clarke': 1, 'Amnesty International': 1, 'Liberty': 1, 'Bristol': 1, \"Scarman's\": 1, 'Woolf': 1, 'Dame Elizabeth Butler-Sloss': 1, 'Spring': 1, 'watchdog': 1, 'the Office of the Deputy': 1, 'Cardiff': 1, 'Multiple Sclerosis': 1}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at unidentified entities in model 2\n",
    "unidentified_entities_text = [ent.text for ent in unidentified_entities]\n",
    "print(get_frequencies(unidentified_entities_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PERSON': 506, 'ORG': 436, 'GPE': 134, 'LOC': 19}\n"
     ]
    }
   ],
   "source": [
    "unidentified_entities_label = [ent.label_ for ent in unidentified_entities]\n",
    "print(get_frequencies(unidentified_entities_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number and type of spurious entities by Model 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Labour': 17, 'Tory': 10, 'TPS': 6, 'Kilroy-Silk': 6, 'Baume': 6, 'Blair': 4, 'Parker Bowles': 4, 'Jeeves': 4, 'TGWU': 4, 'Umbro': 4, 'Dr Gibbons': 4, 'the Lib Dems': 3, 'MSPs': 3, 'Home': 3, 'Hockney': 3, 'MPs': 2, 'MoD': 2, 'Eurozone': 2, 'MP': 2, 'Parliamentary': 2, 'IHRC': 2, 'Royal': 2, 'Archbishop': 2, 'Canterbury': 2, 'NAO': 2, 'Parmalat': 2, 'Straw': 2, 'Malik': 2, 'Chidambaram': 2, 'Midlands': 2, 'Maran': 2, 'FDI': 2, 'Mittal': 2, 'Dr Fox': 2, 'MTFG': 2, 'Westminster': 2, 'Nath': 2, 'NHS': 2, 'Lord Falconer': 2, \"Dr Gibbons'\": 2, 'Alfa': 2, 'Asbos': 2, 'AIG': 1, 'Bewlay': 1, 'Atinc Ozkan': 1, 'Mark Ballard': 1, 'Guantanamo': 1, \"BBC One's Breakfast\": 1, 'Wastealot': 1, 'Wellington': 1, \"Plaid Cymru's\": 1, 'German': 1, 'University College London': 1, 'VW': 1, 'the Mercedes Benz C180 SE': 1, 'The Lib Dems': 1, 'May.': 1, 'Bill': 1, 'Children': 1, 'Margaret Hodge': 1, 'Boothroyd': 1, 'Commons Speaker': 1, 'BBC1': 1, 'Easter': 1, 'Docklands': 1, 'the White City': 1, 'SIFF': 1, 'MSP': 1, \"Holyrood's\": 1, 'Chamber Office': 1, \"Mr McLetchie's\": 1, 'Scottish Labour Party': 1, 'theirfamilies': 1, 'radicallyimproved public services': 1, 'frontbencher John Redwood': 1, 'Park': 1, 'LG Card': 1, 'DZ': 1, 'Kimberly Quinn': 1, 'Alan Budd': 1, 'Mrs Quinn': 1, 'CommunicateResearch': 1, 'New Labour': 1, 'Marr': 1, 'the \"State of Agricultural Commodity Markets 2004': 1, 'Sub-Saharan': 1, 'Africa': 1, 'Health': 1, 'SNP': 1, 'the \"care development group': 1, 'Scottish Executive': 1, 'Sullivan': 1, 'South East Asian': 1, 'Sikhism': 1, 'roti': 1, 'Rajasthan': 1, 'Veritas - Latin': 1, 'Ashfield': 1, 'AWOL': 1, 'the Robert Kilroy-Silk Party': 1, 'This Kilroy-Silk': 1, 'RKS': 1, 'Salford MP': 1, 'Cornwall': 1, 'Daily Telegraph': 1, 'Duke': 1, 'Edinburgh': 1, '[Prince Charles]': 1, 'Mrs Camilla Parker Bowles': 1, 'Kemp': 1, 'Labour Britain': 1, 'Departments': 1, 'Armstrong': 1, 'Bondi': 1, 'Sunday Telegraph': 1, 'Fitzpatrick': 1, 'Frederick': 1, 'David - last year': 1, 'Conrad Black': 1, 'Hollinger International': 1, 'an Election Court': 1, 'High Court': 1, 'Bordesley Green': 1, 'Cojedes state': 1, 'Rings': 1, 'ELAA': 1, 'Transport': 1, 'Heineken and Scottish & Newcastle': 1, 'Yantai city': 1, 'Domecq': 1, 'LVMH': 1, 'Havana Club': 1, 'Dresdner': 1, 'Kleinwort Wasserstein': 1, 'Nick Van den Brul': 1, 'RJ Reynolds Tobacco': 1, 'the Court of Appeals': 1, 'the District of Columbia': 1, 'Reynolds': 1, 'T-Mobile USA': 1, 'Patriots': 1, 'Caroline Lucas': 1, 'MPs - Keith Vaz': 1, 'Bernie Grant': 1, 'INSEE': 1, 'Reuters news agency': 1, 'Ogilvy & Mather': 1, 'Cordiant Communications': 1, 'demerge': 1, 'Hard Rock and Deluxe Films': 1, 'Hard Rock': 1, 'Mecca Bingo': 1, 'Hard Rock Cafes': 1, 'eastern': 1, 'Yeo': 1, 'Davis': 1, 'the Survivors Fund (SURF': 1, 'Greensleeves': 1, \"Transport & General Worker's Union\": 1, 'Nanjing Auto': 1, 'Davies': 1, 'Al-Jazeera Arabic': 1, 'the British National Party(BNP': 1, \"Betts-Green's\": 1, 'Khodorkovsky': 1, 'Rosneft': 1, 'Telecoms': 1, 'Sanjay Mehta': 1, 'Ispat': 1, 'Tory co-': 1, 'Labour Party': 1, 'Leicester': 1, 'ILW': 1, 'DTI': 1, 'the Chevrolet Avalanche, Express': 1, 'Silverade': 1, 'Suburban': 1, 'GMC': 1, 'SRX': 1, 'Pontiac': 1, 'Isuzu': 1, 'Menzies Campbell': 1, 'Commons Oliver Heald': 1, '418bn-rand': 1, 'Brown': 1, 'BBC News 24': 1, \"Super union'\": 1, 'Amicus and the Transport and General Workers Union': 1, 'the Warwick Agreement': 1, 'Lord Janner': 1, 'Lord Goldsmith': 1, 'the Director of Public Prosecutions': 1, 'Commerce and Industry Minister': 1, 'Dr Vancheshwar': 1, 'CB': 1, \"the South Korean's\": 1, 'Oaten': 1, 'Mini and Rolls-Royce': 1, 'gigajoule': 1, 'Spindler': 1, 'West Midlands': 1, 'Tuberculosis': 1, 'Heathrow': 1, 'Gatwick': 1, 'the League Against Cruel Sports': 1, 'Dartmoor': 1, 'the West Country': 1, 'Beaufort Hunt': 1, 'hunstman': 1, 'Goldsmith': 1, 'Newsnight': 1, 'Highland': 1, 'the University of the Highlands and Islands (UHI) Millennium Institute': 1, 'Commercial Services': 1, 'Barber': 1, 'Tory fox': 1, 'social security': 1, 'Oxfam': 1, 'Band Aid': 1, 'Duncan': 1, 'Jowell': 1, 'Ealing North': 1, \"Age Concern's Age Agenda\": 1, 'Times': 1, 'nations': 1, 'Allawi': 1, 'the Ministry of Economy, Trade and Industry': 1, 'the 60-employee Potato Council': 1, 'Racial Equality': 1, 'Save the Children': 1, 'the Medical Foundation': 1, 'the Dai-ichi Life Research Institute': 1, 'G8': 1, 'Eurosceptic party UKIP': 1, 'East Kilbride': 1, 'Scotland': 1, 'Houston': 1, 'RAF': 1, 'Harrison': 1, 'Clarke': 1, 'Fairford Coach Action': 1, 'Amnesty International and Liberty': 1, 'Lord Woolf': 1, 'Elizabeth Butler-Sloss': 1, 'Merseyside': 1, 'carmaking': 1, 'Romeo': 1, \"the Royal Mail's\": 1, 'the Office of the': 1, 'Exeter': 1, 'Home Office': 1, 'Superman': 1, \"the Conservative National Women's Committee\": 1}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at spurious\n",
    "spurious_entities_text = [ent.text for ent in spurious_entites]\n",
    "print(get_frequencies(spurious_entities_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ORG': 177, 'PERSON': 130, 'GPE': 33, 'LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "spurious_entities_label = [ent.label_ for ent in spurious_entites]\n",
    "print(get_frequencies(spurious_entities_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
